{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afc8da83",
   "metadata": {},
   "source": [
    "# Sundial Foundation Model within CTF4Science\n",
    "This notebook contains the code to run Sundial Foundation Model within the CTF4Science framework, following the quickstart guid provided in the official [Github repository](https://github.com/thuml/Sundial/blob/main/examples/quickstart_zero_shot_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dec3f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sriva/miniconda3/envs/ctf-sundial/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/sriva/miniconda3/envs/ctf-sundial/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/sriva/miniconda3/envs/ctf-sundial/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from ctf4science.data_module import load_dataset, parse_pair_ids, get_applicable_plots, get_prediction_timesteps, get_training_timesteps, load_validation_dataset, get_validation_prediction_timesteps\n",
    "from ctf4science.eval_module import evaluate, save_results\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "model_name = 'sundial'\n",
    "device = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModelForCausalLM.from_pretrained('thuml/sundial-base-128m', trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10203e43",
   "metadata": {},
   "source": [
    "## Lorenz Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9398655",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ODE_Lorenz'\n",
    "\n",
    "path_fig = f\"{dataset_name}/\"\n",
    "os.makedirs(path_fig, exist_ok=True)\n",
    "\n",
    "validation = False\n",
    "num_samples = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8635a42",
   "metadata": {},
   "source": [
    "Execute for all pair ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abd638c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pair_id: 1\n",
      "> Standard prediction task, using 1000 forecast length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (10000). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Prediction matrix shape: (1000, 3)\n",
      "> Results: {'short_time': 60.09410006061692, 'long_time': -25.73333333333334}\n",
      " \n",
      "Processing pair_id: 2\n",
      "> Reconstruction task, using 200 context length\n",
      "> Prediction matrix shape: (10000, 3)\n",
      "> Results: {'reconstruction': 45.60054650054103}\n",
      " \n",
      "Processing pair_id: 3\n",
      "> Standard prediction task, using 1000 forecast length\n",
      "> Prediction matrix shape: (1000, 3)\n",
      "> Results: {'long_time': -39.733333333333334}\n",
      " \n",
      "Processing pair_id: 4\n",
      "> Reconstruction task, using 200 context length\n",
      "> Prediction matrix shape: (10000, 3)\n",
      "> Results: {'reconstruction': 55.899638539899364}\n",
      " \n",
      "Processing pair_id: 5\n",
      "> Standard prediction task, using 1000 forecast length\n",
      "> Prediction matrix shape: (1000, 3)\n",
      "> Results: {'long_time': -41.866666666666674}\n",
      " \n",
      "Processing pair_id: 6\n",
      "> Standard prediction task, using 1000 forecast length\n",
      "> Prediction matrix shape: (1000, 3)\n",
      "> Results: {'short_time': 27.544332725946173, 'long_time': 46.4}\n",
      " \n",
      "Processing pair_id: 7\n",
      "> Standard prediction task, using 1000 forecast length\n",
      "> Prediction matrix shape: (1000, 3)\n",
      "> Results: {'short_time': 28.906210137426967, 'long_time': -53.46666666666666}\n",
      " \n",
      "Processing pair_id: 8\n",
      "> Burn-in matrix of size 100, using 1000 forecast length\n",
      "> Prediction matrix shape: (1000, 3)\n",
      "> Results: {'short_time': 36.43281059135715}\n",
      " \n",
      "Processing pair_id: 9\n",
      "> Burn-in matrix of size 100, using 1000 forecast length\n",
      "> Prediction matrix shape: (1000, 3)\n",
      "> Results: {'short_time': 76.62094951718922}\n",
      " \n",
      "> Total execution time: 00:00:10\n"
     ]
    }
   ],
   "source": [
    "execution_time = time.time()\n",
    "\n",
    "for pair_id in range(1,10):\n",
    "\n",
    "    print(f\"Processing pair_id: {pair_id}\")\n",
    "\n",
    "    if validation:\n",
    "        train_data, val_data, init_data = load_validation_dataset(dataset_name, pair_id=pair_id)\n",
    "        forecast_length = get_validation_prediction_timesteps(dataset_name, pair_id).shape[0]\n",
    "    else:\n",
    "        train_data, init_data = load_dataset(dataset_name, pair_id=pair_id)\n",
    "        forecast_length = get_prediction_timesteps(dataset_name, pair_id).shape[0]\n",
    "\n",
    "\n",
    "    if pair_id in [2, 4]:\n",
    "        recon_ctx = 200\n",
    "        # Reconstruction\n",
    "        print(f\"> Reconstruction task, using {recon_ctx} context length\")\n",
    "        train_mat = train_data[0]\n",
    "        train_mat = train_mat[0:recon_ctx,:]\n",
    "        forecast_length = forecast_length - recon_ctx\n",
    "    elif pair_id in [8, 9]:\n",
    "        # Burn-in - Parametric Generalisation\n",
    "        print(f\"> Burn-in matrix of size {init_data.shape[0]}, using {forecast_length} forecast length\")\n",
    "        train_mat = init_data\n",
    "        forecast_length = forecast_length - init_data.shape[0]\n",
    "    else:\n",
    "        # Standard prediction\n",
    "        print(f\"> Standard prediction task, using {forecast_length} forecast length\")\n",
    "        train_mat = train_data[0]\n",
    "\n",
    "    _input_data = torch.tensor(train_mat, dtype=torch.float32).to(device).T\n",
    "\n",
    "    # Get prediction data\n",
    "    pred_data = model.generate(_input_data, max_new_tokens=forecast_length, num_samples=num_samples)\n",
    "\n",
    "    if pair_id in [2, 4, 8, 9]:\n",
    "        pred_mat = np.concatenate([train_mat, pred_data.cpu().numpy().mean(axis=1).T], axis=0)\n",
    "    else:\n",
    "        pred_mat = pred_data.cpu().numpy().mean(axis=1).T\n",
    "\n",
    "    # Evaluate the performance (mean prediction over samples)\n",
    "    results = evaluate(dataset_name, pair_id, pred_mat)\n",
    "\n",
    "    # Save results\n",
    "    print(f\"> Prediction matrix shape: {pred_mat.shape}\")\n",
    "    print(f\"> Results: {results}\")\n",
    "\n",
    "    pickle.dump({\n",
    "        'model_name': model_name,\n",
    "        'dataset_name': dataset_name,\n",
    "        'pair_id': pair_id,\n",
    "        'pred_mat': pred_mat,\n",
    "        'pred_shape': pred_data.shape,\n",
    "        'results': results\n",
    "    }, open(f\"{dataset_name}/pair_{pair_id}_results.pkl\", \"wb\"))\n",
    "\n",
    "    print(' ')\n",
    "\n",
    "execution_time = time.time() - execution_time\n",
    "# Convert to HH:MM:SS format\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = int(execution_time % 60)\n",
    "\n",
    "print(f\"> Total execution time: {hours:02d}:{minutes:02d}:{seconds:02d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d049a1",
   "metadata": {},
   "source": [
    "## Kuramoto-Sivashinsky Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc48d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'PDE_KS'\n",
    "\n",
    "path_fig = f\"{dataset_name}/\"\n",
    "os.makedirs(path_fig, exist_ok=True)\n",
    "\n",
    "validation = False\n",
    "num_samples = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e93251",
   "metadata": {},
   "source": [
    "Let us execute for all pair ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d19ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pair_id: 1\n",
      "> Standard prediction task, using 1000 forecast length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pair_id 1 in batches: 100%|██████████| 3/3 [00:01<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Prediction matrix shape: (1000, 1024)\n",
      "> Results: {'short_time': 0.07413805063144485, 'long_time': 0.00030750364858889156}\n",
      " \n",
      "Processing pair_id: 2\n",
      "> Reconstruction task, using 1000 context length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pair_id 2 in batches: 100%|██████████| 3/3 [00:07<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Prediction matrix shape: (10000, 1024)\n",
      "> Results: {'reconstruction': 4.60880708712782}\n",
      " \n",
      "Processing pair_id: 3\n",
      "> Standard prediction task, using 1000 forecast length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pair_id 3 in batches: 100%|██████████| 3/3 [00:01<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Prediction matrix shape: (1000, 1024)\n",
      "> Results: {'long_time': 7.144790827862124e-05}\n",
      " \n",
      "Processing pair_id: 4\n",
      "> Reconstruction task, using 1000 context length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pair_id 4 in batches: 100%|██████████| 3/3 [00:07<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Prediction matrix shape: (10000, 1024)\n",
      "> Results: {'reconstruction': 5.763553021359103}\n",
      " \n",
      "Processing pair_id: 5\n",
      "> Standard prediction task, using 1000 forecast length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pair_id 5 in batches: 100%|██████████| 3/3 [00:01<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Prediction matrix shape: (1000, 1024)\n",
      "> Results: {'long_time': 0.0004692158975361238}\n",
      " \n",
      "Processing pair_id: 6\n",
      "> Standard prediction task, using 1000 forecast length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pair_id 6 in batches: 100%|██████████| 3/3 [00:01<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Prediction matrix shape: (1000, 1024)\n",
      "> Results: {'short_time': 0.03821716311058765, 'long_time': 0.0001854924966071536}\n",
      " \n",
      "Processing pair_id: 7\n",
      "> Standard prediction task, using 1000 forecast length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pair_id 7 in batches: 100%|██████████| 3/3 [00:01<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Prediction matrix shape: (1000, 1024)\n",
      "> Results: {'short_time': -0.06756134693686189, 'long_time': 0.000428945588781815}\n",
      " \n",
      "Processing pair_id: 8\n",
      "> Burn-in matrix of size 100, using 1000 forecast length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pair_id 8 in batches: 100%|██████████| 3/3 [00:01<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Prediction matrix shape: (1000, 1024)\n",
      "> Results: {'short_time': -49.711427031931876}\n",
      " \n",
      "Processing pair_id: 9\n",
      "> Burn-in matrix of size 100, using 1000 forecast length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pair_id 9 in batches: 100%|██████████| 3/3 [00:01<00:00,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Prediction matrix shape: (1000, 1024)\n",
      "> Results: {'short_time': -45.43654667699752}\n",
      " \n",
      "> Total execution time: 00:00:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "execution_time = time.time()\n",
    "\n",
    "batch_size = 400\n",
    "\n",
    "for pair_id in range(1,10):\n",
    "\n",
    "    print(f\"Processing pair_id: {pair_id}\")\n",
    "\n",
    "    if validation:\n",
    "        train_data, val_data, init_data = load_validation_dataset(dataset_name, pair_id=pair_id)\n",
    "        forecast_length = get_validation_prediction_timesteps(dataset_name, pair_id).shape[0]\n",
    "    else:\n",
    "        train_data, init_data = load_dataset(dataset_name, pair_id=pair_id)\n",
    "        forecast_length = get_prediction_timesteps(dataset_name, pair_id).shape[0]\n",
    "\n",
    "\n",
    "    if pair_id in [2, 4]:\n",
    "        recon_ctx = 1000\n",
    "        # Reconstruction\n",
    "        print(f\"> Reconstruction task, using {recon_ctx} context length\")\n",
    "        train_mat = train_data[0]\n",
    "        train_mat = train_mat[0:recon_ctx,:]\n",
    "        forecast_length = forecast_length - recon_ctx\n",
    "    elif pair_id in [8, 9]:\n",
    "        # Burn-in - Parametric Generalisation\n",
    "        print(f\"> Burn-in matrix of size {init_data.shape[0]}, using {forecast_length} forecast length\")\n",
    "        train_mat = init_data\n",
    "        forecast_length = forecast_length - init_data.shape[0]\n",
    "    else:\n",
    "        # Standard prediction\n",
    "        print(f\"> Standard prediction task, using {forecast_length} forecast length\")\n",
    "        train_mat = train_data[0]\n",
    "\n",
    "    # If GPU is too small, we can sequentially process the input data\n",
    "    spatial_dim = train_mat.shape[-1]\n",
    "    pred_data = np.zeros((spatial_dim, num_samples, forecast_length), dtype=np.float32)\n",
    "\n",
    "    for i in tqdm(range(0, spatial_dim, batch_size), desc=f\"Processing pair_id {pair_id} in batches\"):\n",
    "\n",
    "        _input_data = torch.tensor(train_mat[:, i : (i + batch_size)], dtype=torch.float32).to(device).T\n",
    "        _tmp = model.generate(_input_data, max_new_tokens=forecast_length, num_samples=num_samples)\n",
    "        pred_data[i : (i + batch_size)] = _tmp.cpu().numpy()\n",
    "\n",
    "    if pair_id in [2, 4, 8, 9]:\n",
    "        pred_mat = np.concatenate([train_mat, pred_data.mean(axis=1).T], axis=0)\n",
    "    else:\n",
    "        pred_mat = pred_data.mean(axis=1).T\n",
    "\n",
    "    # Evaluate the performance (mean prediction over samples)\n",
    "    results = evaluate(dataset_name, pair_id, pred_mat)\n",
    "\n",
    "    # Save results\n",
    "    print(f\"> Prediction matrix shape: {pred_mat.shape}\")\n",
    "    print(f\"> Results: {results}\")\n",
    "\n",
    "    pickle.dump({\n",
    "        'model_name': model_name,\n",
    "        'dataset_name': dataset_name,\n",
    "        'pair_id': pair_id,\n",
    "        'pred_mat': pred_mat,\n",
    "        'pred_shape': pred_data.shape,\n",
    "        'results': results\n",
    "    }, open(f\"{dataset_name}/pair_{pair_id}_results.pkl\", \"wb\"))\n",
    "\n",
    "    print(' ')\n",
    "\n",
    "execution_time = time.time() - execution_time\n",
    "\n",
    "# Convert to HH:MM:SS format\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = int(execution_time % 60)\n",
    "\n",
    "print(f\"> Total execution time: {hours:02d}:{minutes:02d}:{seconds:02d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695eb274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctf-sundial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
